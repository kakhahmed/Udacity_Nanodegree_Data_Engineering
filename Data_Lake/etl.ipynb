{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Lake Project\n",
    "\n",
    "Author: Karim Ahmed\n",
    "\n",
    "\n",
    "This notebook runs on AWS EMR cluster in the `us-west-2` region using pyspark kernel.\n",
    "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-notebooks.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import (\n",
    "    year,\n",
    "    month,\n",
    "    dayofmonth,\n",
    "    hour,\n",
    "    weekofyear,\n",
    "    dayofweek,\n",
    "    date_format,\n",
    "    monotonically_increasing_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Input and output S3s\n",
    "\n",
    "input_data = \"s3a://udacity-dend/\"\n",
    "output_data = \"s3a://aws-bucket-udacity-de/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process song data\n",
    "\n",
    "Create and store `songs_table` and `artists_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# read song data file\n",
    "song_data = input_data + \"song_data/*/*/*/*.json\"\n",
    "song_df = spark.read.json(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# extract columns to create songs table\n",
    "# song_id, title, artist_id, year, duration\n",
    "songs_table = song_df.select(\n",
    "    'song_id', 'title', 'artist_id', 'year', 'duration')\n",
    "\n",
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_table.write.mode(\"overwrite\").partitionBy(\n",
    "    \"artist_id\", \"year\").parquet(output_data + \"/songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# extract columns to create artists table\n",
    "# artist_id, name, location, lattitude, longitude\n",
    "artists_table = song_df.select(\n",
    "    'artist_id',\n",
    "    'artist_name',\n",
    "    'artist_location',\n",
    "    'artist_latitude',\n",
    "    'artist_longitude',\n",
    ").dropDuplicates()\n",
    "\n",
    "# write artists table to parquet files\n",
    "artists_table.write.mode(\"overwrite\").parquet(output_data + \"/artists\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process log data\n",
    "\n",
    "Create and store `users_table` and `time_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "log_data = input_data + \"log_data/*/*/*.json\"\n",
    "\n",
    "# read log data file\n",
    "log_df = spark.read.json(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# filter by actions for song plays\n",
    "log_df = log_df.filter(log_df.page==\"NextSong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# extract columns for users table \n",
    "# user_id, first_name, last_name, gender, level\n",
    "users_table = log_df.select(\n",
    "    col(\"userId\"),\n",
    "    col(\"firstName\"),\n",
    "    col(\"lastName\"),\n",
    "    col(\"gender\"),\n",
    "    col(\"level\"),\n",
    ").dropDuplicates()\n",
    "\n",
    "# write users table to parquet files\n",
    "users_table.write.mode(\"overwrite\").parquet(output_data + \"/users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# create timestamp column from original timestamp column\n",
    "get_timestamp = udf(lambda x: datetime.fromtimestamp(int(x)/1000).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "log_df = log_df.withColumn('timestamp', get_timestamp(log_df.ts))\n",
    "\n",
    "# create datetime column from original timestamp column\n",
    "get_datetime = udf(lambda x: str(datetime.fromtimestamp(int(x) / 1000.0)))\n",
    "log_df = log_df.withColumn('datetime', get_datetime(log_df.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# extract columns to create time table\n",
    "time_table = log_df.select('timestamp).withColumn(\n",
    "    'start_time', lod_df.timestamp\n",
    ").withColumn(\n",
    "    'hour', dayofmonth('timestamp')\n",
    ").withColumn(\n",
    "    'day', dayofmonth('timestamp')\n",
    ").withColumn(\n",
    "    'week', weekofyear('timestamp')\n",
    ").withColumn(\n",
    "    'month', month('timestamp')\n",
    ").withColumn(\n",
    "    'year', year('timestamp')\n",
    ").withColumn(\n",
    "    'weekday', dayofweek('timestamp')\n",
    ").dropDuplicates()\n",
    "\n",
    "# write time table to parquet files partitioned by year and month\n",
    "time_table.write.partitionBy(\n",
    "        \"year\", \"month\").mode(\"overwrite\").parquet(output_data + \"/time_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Songplays_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# extract columns from joined song and log datasets\n",
    "# to create songplays table\n",
    "log_df = log_df.alias('log_df')\n",
    "song_df = song_df.alias('song_df')\n",
    "songplays_table = log_df.join(\n",
    "    song_df, col('log_df.artist') == col('song_df.artist_name')).select(\n",
    "    col(\"log_df.datetime\").alias(\"start_time\"),\n",
    "    col(\"log_df.userId\").alias(\"user_id\"),\n",
    "    col(\"log_df.level\"),\n",
    "    col(\"song_df.song_id\"),\n",
    "    col(\"song_df.artist_id\"),\n",
    "    col(\"log_df.sessionId\").alias(\"session_id\"),\n",
    "    col(\"log_df.location\").alias(\"location\"),\n",
    "    col(\"log_df.userAgent\").alias(\"user_agent\"),\n",
    "    month('log_df.datetime').alias('month'),\n",
    "    year('log_df.datetime').alias('year'),\n",
    ").withColumn('songplay_id', monotonically_increasing_id()\n",
    ").dropDuplicates()\n",
    "\n",
    "\n",
    "# write songplays table to parquet files partitioned by year and month\n",
    "songplays_table.write.partitionBy(\n",
    "    \"year\", \"month\").mode(\n",
    "    \"overwrite\").parquet(output_data + \"/songplays_table\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
