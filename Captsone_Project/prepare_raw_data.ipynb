{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare raw dataset for Capstone project\n",
    "\n",
    "Notebook used to create the JSON datasource of the project from `football_data_transfer_market`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/13 01:25:25 WARN Utils: Your hostname, exflqr43268 resolves to a loopback address: 127.0.1.1; using 192.168.0.215 instead (on interface wlp59s0)\n",
      "23/04/13 01:25:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/13 01:25:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/13 01:25:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col, when\n",
    "import etl\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert SQLITE to JSON dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test number of combined rows for both dataset sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "competitions 43\n",
      "games 61197\n",
      "clubs 401\n",
      "players 27456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appearances 1049585\n",
      "club_games 122394\n",
      "`football_data_transfer_market` rows count: 1261076\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "directory_path = Path(\"football_data_transfer_market\")\n",
    "\n",
    "csv_files = directory_path.glob(\"*.csv\")\n",
    "total_count = 0\n",
    "count = 0\n",
    "for csv in csv_files:\n",
    "    if \"player_valuations\" in str(csv) or \"events\" in str(csv):\n",
    "        continue\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(str(csv))\n",
    "    df_count = df.count()\n",
    "    count += df_count\n",
    "    print(csv.stem, df_count)\n",
    "print(\"`football_data_transfer_market` rows count:\", count)\n",
    "total_count += count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# This can be added into a json separate dataset.\n",
    "players_df = etl.read_csv(\"football_data_transfer_market/players.csv\", spark)\n",
    "player_appearances = etl.read_csv(\"football_data_transfer_market/appearances.csv\", spark)\n",
    "game_events = etl.read_csv(\"football_data_transfer_market/game_events.csv\", spark)\n",
    "\n",
    "df = players_df.join(player_appearances, \"player_id\", \"left\").join(game_events, [\"player_id\", \"game_id\"], \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the data in JSON format\n",
    "df.write.format(\"json\").save(\"raw_data/players_appearances\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00f37813591412101a92758a43996fb3846e9344c8651eda490de8a230469afd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
